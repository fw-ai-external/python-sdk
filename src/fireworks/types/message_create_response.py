# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import List, Union, Optional
from typing_extensions import Literal

from .._models import BaseModel
from .content_block import ContentBlock, ContentBlock as ContentBlock

__all__ = ["MessageCreateResponse", "RawOutput"]


class RawOutput(BaseModel):
    """
    Fireworks extension that returns low-level details of what the model sees, including the formatted prompt and function calls.
    """

    completion: str
    """Raw completion produced by the model before any tool calls are parsed"""

    prompt_fragments: List[Union[str, int]]
    """
    Pieces of the prompt (like individual messages) before truncation and
    concatenation. Depending on prompt_truncate_len some of the messages might be
    dropped. Contains a mix of strings to be tokenized and individual tokens (if
    dictated by the conversation template)
    """

    prompt_token_ids: List[int]
    """Fully processed prompt as seen by the model"""

    completion_token_ids: Optional[List[int]] = None
    """Token IDs for the raw completion"""

    grammar: Optional[str] = None
    """
    Grammar used for constrained decoding, can be either user provided (directly or
    JSON schema) or inferred by the chat template
    """

    images: Optional[List[str]] = None
    """Images in the prompt"""


class MessageCreateResponse(BaseModel):
    id: str
    """Unique object identifier.

    The format and length of IDs may change over time.
    """

    content: List[ContentBlock]
    """Content generated by the model.

    This is an array of content blocks, each of which has a `type` that determines
    its shape.

    Example:

    ```json
    [{ "type": "text", "text": "Hi, I'm here to help." }]
    ```

    If the request input `messages` ended with an `assistant` turn, then the
    response `content` will continue directly from that last turn. You can use this
    to constrain the model's output.

    For example, if the input `messages` were:

    ```json
    [
      {
        "role": "user",
        "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"
      },
      { "role": "assistant", "content": "The best answer is (" }
    ]
    ```

    Then the response `content` might be:

    ```json
    [{ "type": "text", "text": "B)" }]
    ```
    """

    model: str
    """The model that will complete your prompt.

    See the [Fireworks Model Library](https://app.fireworks.ai/models) for available
    models.
    """

    role: Literal["assistant"]
    """Conversational role of the generated message.

    This will always be `"assistant"`.
    """

    stop_reason: Optional[Literal["end_turn", "max_tokens", "stop_sequence", "tool_use", "pause_turn", "refusal"]] = (
        None
    )
    """The reason that the model stopped.

    This may be one the following values:

    - `"end_turn"`: the model reached a natural stopping point
    - `"max_tokens"`: the model exceeded the requested `max_tokens` or the model's
      maximum
    - `"stop_sequence"`: one of your provided custom `stop_sequences` was generated
    - `"tool_use"`: the model invoked one or more tools
    - `"pause_turn"`: the model paused a long-running turn. You may provide the
      response back as-is in a subsequent request to let the model continue.
    - `"refusal"`: when streaming classifiers intervene to handle potential policy
      violations

    In non-streaming mode this value is always non-null. In streaming mode, it is
    null in the `message_start` event and non-null otherwise.
    """

    stop_sequence: Optional[str] = None
    """Which custom stop sequence was generated, if any.

    This value will be a non-null string if one of your custom stop sequences was
    generated.
    """

    type: Literal["message"]
    """Object type.

    For Messages, this is always `"message"`.
    """

    raw_output: Optional[RawOutput] = None
    """
    Fireworks extension that returns low-level details of what the model sees,
    including the formatted prompt and function calls.
    """
